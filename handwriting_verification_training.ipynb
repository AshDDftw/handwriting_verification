{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "X7qP_OpR_BvC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H-6pb2MPG2Oe"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "VHdWyxsp_BvE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n",
        "def create_model(batch_size,epochs,X_train_pairs,y_train_pairs,X_val_pairs,y_val_pairs):\n",
        "\n",
        "    img_A_inp = Input((64, 64), name='img_A_inp')\n",
        "    img_B_inp = Input((64, 64), name='img_B_inp')\n",
        "\n",
        "    def get_cnn_block(depth):\n",
        "        return Sequential([\n",
        "            Conv2D(depth, 3, 1),\n",
        "            BatchNormalization(),\n",
        "            ReLU()\n",
        "        ])\n",
        "\n",
        "    DEPTH = 64\n",
        "    cnn = Sequential([\n",
        "        Reshape((64, 64, 1)),\n",
        "        get_cnn_block(DEPTH),\n",
        "        get_cnn_block(DEPTH * 2),\n",
        "        get_cnn_block(DEPTH * 4),\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(64, activation='relu')\n",
        "    ])\n",
        "\n",
        "    feature_vector_A = cnn(img_A_inp)\n",
        "    feature_vector_B = cnn(img_B_inp)\n",
        "\n",
        "    concat = Concatenate()([feature_vector_A, feature_vector_B])\n",
        "\n",
        "    dense = Dense(64, activation='relu')(concat)\n",
        "    dropout = Dropout(0.5)(dense)  # Add dropout regularization\n",
        "    output = Dense(1, activation='sigmoid')(dropout)\n",
        "\n",
        "    model = Model(inputs=[img_A_inp, img_B_inp], outputs=output)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    es = EarlyStopping(patience=3)\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x=[X_train_pairs[:, 0, :, :], X_train_pairs[:, 1, :, :]],\n",
        "          y=y_train_pairs,\n",
        "          validation_data=([X_val_pairs[:, 0, :, :],\n",
        "                            X_val_pairs[:, 1, :, :]],\n",
        "                           y_val_pairs),\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          callbacks=[es])\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSLqy0JPHBEy"
      },
      "source": [
        "**function to create paired images dataset and corresponding label**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "onE_RlcQ_BvG"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def make_paired_dataset(X, y):\n",
        "  X_pairs, y_pairs = [], []\n",
        "\n",
        "  tuples = [(x1, y1) for x1, y1 in zip(X, y)]\n",
        "\n",
        "  for t in itertools.product(tuples, tuples):\n",
        "    pair_A, pair_B = t\n",
        "    img_A, label_A = pair_A\n",
        "    img_B, label_B = pair_B\n",
        "\n",
        "    new_label = int(label_A == label_B)\n",
        "\n",
        "    X_pairs.append([img_A, img_B])\n",
        "    y_pairs.append(new_label)\n",
        "\n",
        "  X_pairs = np.array(X_pairs)\n",
        "  y_pairs = np.array(y_pairs)\n",
        "\n",
        "  # Reshape X_pairs to match the desired shape (40000, 2, 64, 64)\n",
        "  # X_pairs = np.array(X_pairs.tolist()).reshape(-1, 2, 64, 64)\n",
        "\n",
        "  return X_pairs, y_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39qQ2asHHT0"
      },
      "source": [
        "**Creating training and validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "wwXRBVJ-_BvH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def createDataset(train_data_dir,size):\n",
        "\n",
        "    # Set the path to your dataset directory\n",
        "    dataset_path = train_data_dir\n",
        "\n",
        "    # Get a list of all folders (person IDs) in the dataset\n",
        "    person_folders = os.listdir(dataset_path)\n",
        "\n",
        "    # Initialize an empty list to store the dataset rows\n",
        "    dataset = []\n",
        "\n",
        "    for i in range(0,size):\n",
        "\n",
        "        # Randomly select two folders\n",
        "        selected_folders = random.sample(person_folders, 5)\n",
        "\n",
        "        # print(selected_folders)\n",
        "\n",
        "    # Iterate over each selected folder\n",
        "    # for folder in selected_folders:\n",
        "        # Get the list of image filenames in the current folder\n",
        "        image_files_1 = os.listdir(os.path.join(dataset_path, selected_folders[0]))\n",
        "\n",
        "        # image_files_2 = os.listdir(os.path.join(dataset_path, selected_folders[0]))\n",
        "\n",
        "        # Randomly select two images from the current folder\n",
        "        # print(image_files_1)\n",
        "        image_1 = random.sample(image_files_1,3)\n",
        "        # image_2 = random.sample(image_files_2, 1)\n",
        "        # print(selected_folders)\n",
        "\n",
        "        image = cv2.imread(dataset_path+'/'+selected_folders[0]+'/'+image_1[0], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        reshaped_image=cv2.resize(image, (64,64),interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "        # Create a row for the dataset with image paths and label\n",
        "        row = {\n",
        "            \"img_1\": reshaped_image.reshape(64,64),\n",
        "\n",
        "            \"label\": dataset_path+'/'+selected_folders[0]+'/'+image_1[0]\n",
        "            }\n",
        "\n",
        "        # Append the row to the dataset\n",
        "        dataset.append(row)\n",
        "\n",
        "    # Convert the dataset list to a Pandas DataFrame\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # Save the dataset to a CSV file\n",
        "    # df.to_csv(\"train_dataset_real.csv\", index=False)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qojej_AgHRdk"
      },
      "source": [
        "**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FBpOM0QP_BvI"
      },
      "outputs": [],
      "source": [
        "def train_model(batch_size, epochs,dataset_path,test_size,val_size):\n",
        "\n",
        "    df_train=createDataset(dataset_path,test_size)\n",
        "    df_val=createDataset(dataset_path,val_size)\n",
        "\n",
        "    X_train_pairs, y_train_pairs = make_paired_dataset(df_train.iloc[:,0],df_train.iloc[:,1])\n",
        "    X_val_pairs, y_val_pairs = make_paired_dataset(df_val.iloc[:,0],df_val.iloc[:,1])\n",
        "\n",
        "    model=create_model(batch_size,epochs,X_train_pairs,y_train_pairs,X_val_pairs,y_val_pairs)\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayKpDs5yHW7g"
      },
      "source": [
        "**Function to validate the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XiFHlHvY_BvJ"
      },
      "outputs": [],
      "source": [
        "def validate_model(model,val_path,validation_images_file):\n",
        "    val=pd.read_csv(val_path)\n",
        "    rows,columns=val.shape\n",
        "\n",
        "    dataset=[]\n",
        "    values=[]\n",
        "    labels=[]\n",
        "    pred=[]\n",
        "\n",
        "\n",
        "    for row in range(rows):\n",
        "        path1=val.iloc[row,0]\n",
        "        path2=val.iloc[row,1]\n",
        "        img1=cv2.imread(validation_images_file+'/'+path1,cv2.IMREAD_GRAYSCALE)\n",
        "        img2=cv2.imread(validation_images_file+'/'+path2,cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        imgA=cv2.resize(img1,(64,64),interpolation=cv2.INTER_LANCZOS4)\n",
        "        imgB=cv2.resize(img2,(64,64),interpolation=cv2.INTER_LANCZOS4)\n",
        "        value=model.predict([imgA.reshape((1, 64,64)),imgB.reshape((1, 64,64))]).flatten()[0]\n",
        "        label=val.iloc[row,2]\n",
        "        values.append(value)\n",
        "\n",
        "        labels.append(label)\n",
        "        if(label>0.0001):\n",
        "          pred.append(1)\n",
        "        else:\n",
        "          pred.append(0)\n",
        "        # Create a row for the dataset with image paths and label\n",
        "        row = {\n",
        "            \"img1_name\": path1,\n",
        "\n",
        "            'img2_name':path2,\n",
        "\n",
        "            \"label\": pred[row],\n",
        "\n",
        "            'proba':values[row]\n",
        "\n",
        "\n",
        "            }\n",
        "\n",
        "        # Append the row to the dataset\n",
        "        dataset.append(row)\n",
        "\n",
        "    # Convert the dataset list to a Pandas DataFrame\n",
        "    result = pd.DataFrame(dataset)\n",
        "\n",
        "    # Save the dataset to a CSV file\n",
        "    result.to_csv(\"result.csv\", index=False)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Gcl9DvHY_BvK"
      },
      "outputs": [],
      "source": [
        "def create_new_model(batch_size,epochs,train_dataset_path,validation_csv_path,validation_images_file,train_size,val_size):\n",
        "  model=train_model(batch_size, epochs,train_dataset_path,train_size,val_size)\n",
        "  model.save('saved_model')\n",
        "  validation_results=validate_model(model,validation_csv_path,validation_images_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "iuzxFK6DA6Ua"
      },
      "outputs": [],
      "source": [
        "# model.save('saved_model')\n",
        "# validation_results=validate_model(model,'/content/gdrive/MyDrive/dataset/val.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
